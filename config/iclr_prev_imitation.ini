[DEFAULT]
# number of times to repeat the experiment (to compute averages, etc)
n_repeat = 1
# number of episodes per experiment
n_episodes = 500
# number of time steps per episode
n_steps = 500
# name of the experiment
run_id = iclr_prev_imitation
# simulation modes: computer vision (cv) or using the pixhawk (px4)
mode = cv
# enable use of gui showing uab view
use_gui = 0

# select the desired agent for the experiment
# - human (control agent using joystick)
# - imitation (pre-trained behavior cloning network)
# - ddpg (deep rl algorithm, deep deterministic policy gradient)
agent = imitation

# reward signal
#   0 - evaluation (handcrafted reward)
#   1 - cybersteer_1
#   2 - cybersteer_2
reward_signal = 0

[DDPG]
# define specific parameters for the DDPG learning agents

# learning rate for the actor network
actor_lrate = 0.0001
# learning rate for the critic network
critic_lrate = 0.001
# discount factor
gamma = 0.99
# soft target update param
tau = 0.001
# random action probability
eps = .1
# size of the batch during experience replay
batch_size = 16
# size of the experience replay buffer
buffer_size = 100000
# time steps between each target update
target_update_freq = 100
# if experience buffer should be pre-filled with random experience beforehand
pre_fill_buffer = 0
# if want to use target networks
target = 0
# episodes between each independent agent evaluation
eval_factor = 100
# number of frames stacked together per state
n_frames = 3


[PLOT]
# define plot configurations
